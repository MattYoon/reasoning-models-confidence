{
       "cells": [
              {
                     "cell_type": "code",
                     "execution_count": null,
                     "metadata": {},
                     "outputs": [],
                     "source": [
                            "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
                            "import numpy as np\n",
                            "import json\n",
                            "import pandas as pd\n",
                            "import os\n",
                            "\n",
                            "\n",
                            "def get_full_path(path):\n",
                            "    # List all files (ignore directories)\n",
                            "    files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
                            "    \n",
                            "    if len(files) != 1:\n",
                            "        raise ValueError(f\"Expected exactly one file in {path}, but found {len(files)} files.\")\n",
                            "    \n",
                            "    return os.path.join(path, files[0])\n",
                            "\n",
                            "def read_data(path):\n",
                            "    full_path = get_full_path(path)\n",
                            "    data = json.load(open(full_path))\n",
                            "    # data.keys()\n",
                            "    data_key = list(data['results'].keys())[0]\n",
                            "    data_ = data['results'][data_key]['examples']\n",
                            "    df = pd.DataFrame(data_)\n",
                            "    df['model_confidence'] = df['model_confidence'].apply(clean_confidence)\n",
                            "\n",
                            "    df[\"confidence_score\"] = df[\"model_confidence\"].map(confidence_mapping)\n",
                            "\n",
                            "    return df\n",
                            "\n",
                            "def calc_metrics(df):\n",
                            "    accuracy = sum(df[\"correct\"].values) / len(df)\n",
                            "    df_faulty = df[~df['model_confidence'].isin(class_names)]\n",
                            "    df_clean = df.dropna(subset=[\"confidence_score\", \"correct\"])\n",
                            "\n",
                            "    probs = df_clean[\"confidence_score\"].values\n",
                            "    labels = df_clean[\"correct\"].values\n",
                            "\n",
                            "\n",
                            "    ece = compute_ece(probs, labels)\n",
                            "    auroc = roc_auc_score(labels, probs)\n",
                            "    brier = brier_score_loss(labels, probs)\n",
                            "\n",
                            "    return {\n",
                            "        \"Faulty\": len(df_faulty),\n",
                            "        \"Accuracy\": accuracy,\n",
                            "        \"ECE\": ece,\n",
                            "        \"Brier Score\": brier,\n",
                            "        \"AUROC\": auroc\n",
                            "    }\n",
                            "\n",
                            "def clean_confidence(x):\n",
                            "    if x is None:\n",
                            "        return None\n",
                            "    if x.startswith('{'):\n",
                            "        x = x[1:]\n",
                            "    if '\\\\' in x:\n",
                            "        # remove the escape character\n",
                            "        x = x.replace('\\\\', '')\n",
                            "    x = x.replace('$', '')\n",
                            "    x = x.replace('\"', '')\n",
                            "    x = x.replace('_', ' ')\n",
                            "    \n",
                            "\n",
                            "    x = x.replace('text{', '')\n",
                            "\n",
                            "    if 'slight' in x:\n",
                            "        return 'chances are slight'\n",
                            "    \n",
                            "\n",
                            "    return x.lower() if x else x\n",
                            "\n",
                            "def compute_ece(probs, labels, n_bins=10):\n",
                            "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
                            "    bin_indices = np.digitize(probs, bins) - 1\n",
                            "    ece = 0.0\n",
                            "    for i in range(n_bins):\n",
                            "        bin_mask = bin_indices == i\n",
                            "        if np.any(bin_mask):\n",
                            "            bin_conf = np.mean(probs[bin_mask])\n",
                            "            bin_acc = np.mean(labels[bin_mask])\n",
                            "            bin_frac = np.sum(bin_mask) / len(labels)\n",
                            "            ece += bin_frac * abs(bin_conf - bin_acc)\n",
                            "    return ece\n",
                            "\n",
                            "def token_length(text, tokenizer):\n",
                            "    tokens = tokenizer.tokenize(text)\n",
                            "    return len(tokens)\n",
                            "\n",
                            "class_names = [\n",
                            "    'almost no chance',\n",
                            "    'highly unlikely',\n",
                            "    'chances are slight',\n",
                            "    'unlikely',\n",
                            "    'less than even',\n",
                            "    'better than even',\n",
                            "    'likely',\n",
                            "    'very good chance',\n",
                            "    'highly likely',\n",
                            "    'almost certain'\n",
                            "]\n",
                            "\n",
                            "\n",
                            "# Confidence mapping\n",
                            "confidence_mapping = {\n",
                            "    \"almost no chance\": 0.05,\n",
                            "    \"highly unlikely\": 0.15,\n",
                            "    \"chances are slight\": 0.25,\n",
                            "    \"unlikely\": 0.35,\n",
                            "    \"less than even\": 0.45,\n",
                            "    \"better than even\": 0.55,\n",
                            "    \"likely\": 0.65,\n",
                            "    \"very good chance\": 0.75,\n",
                            "    \"highly likely\": 0.85,\n",
                            "    \"almost certain\": 0.95\n",
                            "}\n"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": null,
                     "metadata": {},
                     "outputs": [],
                     "source": [
                            "df = read_data('/home/dongkeun/reasoning-models-confidence/evalchemy/logs/reasoning_slope/r1-triviaqa-slope/deepseek-ai__DeepSeek-R1-Distill-Qwen-32B')\n",
                            "df.head(1)"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": null,
                     "metadata": {},
                     "outputs": [],
                     "source": [
                            "# split the dfs using the value of 'prompt_pct column\n",
                            "dfs = [df_subset for _, df_subset in df.groupby('prompt_pct')]\n",
                            "dfs_metrics = [calc_metrics(df_subset) for df_subset in dfs]\n",
                            "\n",
                            "# print the metrics\n",
                            "full_results = {\n",
                            "    \"Accuracy\": [],\n",
                            "    \"ECE\": [],\n",
                            "    \"Brier Score\": [],\n",
                            "    \"AUROC\": []\n",
                            "}\n",
                            "for df in dfs:\n",
                            "    metrics = calc_metrics(df)\n",
                            "    # print all the metrics in a single line\n",
                            "    for metric, value in metrics.items():\n",
                            "        if metric != \"Faulty\":\n",
                            "            print(value, end=\", \")\n",
                            "            full_results[metric].append(value)\n",
                            "    print()"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": null,
                     "metadata": {},
                     "outputs": [],
                     "source": [
                            "from scipy.stats import linregress\n",
                            "\n",
                            "# calculate the slope of each metric using linear regression\n",
                            "for metric, values in full_results.items():\n",
                            "    x = list(range(len(values)))\n",
                            "    y = values\n",
                            "\n",
                            "    slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
                            "    # print until the fourth decimal\n",
                            "    slope = round(slope, 3)\n",
                            "    p_value = round(p_value, 3)\n",
                            "    p_value_is_significant = p_value < 0.05\n",
                            "    print(slope, end=\", \")\n",
                            "    print(p_value_is_significant, end=\", \")\n",
                            "    "
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": null,
                     "metadata": {},
                     "outputs": [],
                     "source": []
              }
       ],
       "metadata": {
              "kernelspec": {
                     "display_name": "test",
                     "language": "python",
                     "name": "python3"
              },
              "language_info": {
                     "codemirror_mode": {
                            "name": "ipython",
                            "version": 3
                     },
                     "file_extension": ".py",
                     "mimetype": "text/x-python",
                     "name": "python",
                     "nbconvert_exporter": "python",
                     "pygments_lexer": "ipython3",
                     "version": "3.11.11"
              }
       },
       "nbformat": 4,
       "nbformat_minor": 2
}
