{
       "cells": [
              {
                     "cell_type": "code",
                     "execution_count": 1,
                     "metadata": {},
                     "outputs": [],
                     "source": [
                            "all_paths = [\n",
                            "            ['/home/dongkeun/reasoning-models-confidence/evalchemy/logs/non_reasoning/triviaqa_val_1k/Qwen__Qwen2.5-32B-Instruct'],\n",
                            "            # ['/home/dongkeun/reasoning-models-confidence/evalchemy/logs/reasoning_force/triviaqa_val_1k/deepseek-ai__DeepSeek-R1-Distill-Qwen-32B'],\n",
                            "             ]"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": null,
                     "metadata": {},
                     "outputs": [],
                     "source": [
                            "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
                            "import numpy as np\n",
                            "import json\n",
                            "import pandas as pd\n",
                            "import os\n",
                            "\n",
                            "def get_full_path(path):\n",
                            "    # List all files (ignore directories)\n",
                            "    files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
                            "    \n",
                            "    if len(files) != 1:\n",
                            "        raise ValueError(f\"Expected exactly one file in {path}, but found {len(files)} files.\")\n",
                            "    \n",
                            "    return os.path.join(path, files[0])\n",
                            "\n",
                            "def get_results(path):\n",
                            "    path = get_full_path(path)\n",
                            "    data = json.load(open(path))\n",
                            "    # data.keys()\n",
                            "    data_key = list(data['results'].keys())[0]\n",
                            "    data_ = data['results'][data_key]['examples']\n",
                            "    df = pd.DataFrame(data_)\n",
                            "    df['model_confidence'] = df['model_confidence'].apply(clean_confidence)\n",
                            "\n",
                            "    df_faulty = df[~df['model_confidence'].isin(class_names)]\n",
                            "\n",
                            "    df[\"confidence_score\"] = df[\"model_confidence\"].map(confidence_mapping)\n",
                            "    df_clean = df.dropna(subset=[\"confidence_score\", \"correct\"])\n",
                            "\n",
                            "\n",
                            "    probs = df_clean[\"confidence_score\"].values\n",
                            "    labels = df_clean[\"correct\"].values\n",
                            "\n",
                            "    ece = compute_ece(probs, labels)\n",
                            "    auroc = roc_auc_score(labels, probs)\n",
                            "    brier = brier_score_loss(labels, probs)\n",
                            "\n",
                            "    accuracy = data['results'][data_key]['accuracy']\n",
                            "\n",
                            "    return {\n",
                            "        \"Faulty\": len(df_faulty),\n",
                            "        \"Accuracy\": accuracy,\n",
                            "        \"ECE\": ece,\n",
                            "        \"Brier Score\": brier,\n",
                            "        \"AUROC\": auroc\n",
                            "    }\n",
                            "\n",
                            "\n",
                            "def clean_confidence(x):\n",
                            "    if x is None:\n",
                            "        return None\n",
                            "    if x.startswith('{'):\n",
                            "        x = x[1:]\n",
                            "    if '\\\\' in x:\n",
                            "        # remove the escape character\n",
                            "        x = x.replace('\\\\', '')\n",
                            "    x = x.replace('$', '')\n",
                            "    x = x.replace('\"', '')\n",
                            "    x = x.replace('_', ' ')\n",
                            "    \n",
                            "\n",
                            "    x = x.replace('text{', '')\n",
                            "\n",
                            "    if 'slight' in x:\n",
                            "        return 'chances are slight'\n",
                            "    \n",
                            "\n",
                            "    return x.lower() if x else x\n",
                            "\n",
                            "def compute_ece(probs, labels, n_bins=10):\n",
                            "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
                            "    bin_indices = np.digitize(probs, bins) - 1\n",
                            "    ece = 0.0\n",
                            "    for i in range(n_bins):\n",
                            "        bin_mask = bin_indices == i\n",
                            "        if np.any(bin_mask):\n",
                            "            bin_conf = np.mean(probs[bin_mask])\n",
                            "            bin_acc = np.mean(labels[bin_mask])\n",
                            "            bin_frac = np.sum(bin_mask) / len(labels)\n",
                            "            ece += bin_frac * abs(bin_conf - bin_acc)\n",
                            "    return ece\n",
                            "\n",
                            "class_names = [\n",
                            "    'almost no chance',\n",
                            "    'highly unlikely',\n",
                            "    'chances are slight',\n",
                            "    'unlikely',\n",
                            "    'less than even',\n",
                            "    'better than even',\n",
                            "    'likely',\n",
                            "    'very good chance',\n",
                            "    'highly likely',\n",
                            "    'almost certain'\n",
                            "]\n",
                            "\n",
                            "\n",
                            "# Confidence mapping\n",
                            "confidence_mapping = {\n",
                            "    \"almost no chance\": 0.05,\n",
                            "    \"highly unlikely\": 0.15,\n",
                            "    \"chances are slight\": 0.25,\n",
                            "    \"unlikely\": 0.35,\n",
                            "    \"less than even\": 0.45,\n",
                            "    \"better than even\": 0.55,\n",
                            "    \"likely\": 0.65,\n",
                            "    \"very good chance\": 0.75,\n",
                            "    \"highly likely\": 0.85,\n",
                            "    \"almost certain\": 0.95\n",
                            "}\n"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 3,
                     "metadata": {},
                     "outputs": [],
                     "source": [
                            "all_results = []\n",
                            "\n",
                            "for paths in all_paths:\n",
                            "    results = []\n",
                            "    for path in paths:\n",
                            "        result = get_results(path)\n",
                            "        results.append(result)\n",
                            "    all_results.append(results)"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 4,
                     "metadata": {},
                     "outputs": [
                            {
                                   "name": "stdout",
                                   "output_type": "stream",
                                   "text": [
                                          "0.8, 0.20000000000000007, 0.07450000000000001, 1.0, \n"
                                   ]
                            }
                     ],
                     "source": [
                            "for results in all_results:\n",
                            "    for result in results:\n",
                            "        print(\n",
                            "            result['Accuracy'],\n",
                            "            result['ECE'],\n",
                            "            result['Brier Score'],\n",
                            "            result['AUROC'],\n",
                            "            sep=', ',\n",
                            "            end=', '\n",
                            "        )   \n",
                            "    print()"
                     ]
              }
       ],
       "metadata": {
              "kernelspec": {
                     "display_name": "test",
                     "language": "python",
                     "name": "python3"
              },
              "language_info": {
                     "codemirror_mode": {
                            "name": "ipython",
                            "version": 3
                     },
                     "file_extension": ".py",
                     "mimetype": "text/x-python",
                     "name": "python",
                     "nbconvert_exporter": "python",
                     "pygments_lexer": "ipython3",
                     "version": "3.11.11"
              }
       },
       "nbformat": 4,
       "nbformat_minor": 2
}
